
<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <title>Model Composition</title>
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <link rel="icon" type="image/x-icon" href="/assets/static/logo.jpg">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#e77500">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css">

    <!-- MathJax -->
    <script
      type="text/javascript"
      async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"
    >
    </script>

    <!-- OG cards, including feishu, facebook, etc. -->
    <meta name="og:title"   content="Model Composition">
    <meta name="og:image"   content="https://fzacker.github.io/model-composition.github.io/assets/static/preview-card.png">
    <meta name="og:description" content="Model Composition for Multimodal Large Language Models">
    <meta name="og:url" content="https://fzacker.github.io/model-composition.github.io/model-composition.html">

    <!-- Twitter cards -->
    <meta name="twitter:title"   content="Model Composition">
    <meta name="twitter:card"    content="summary_large_image">
    <meta name="twitter:image"   content="https://fzacker.github.io/model-composition.github.io/assets/static/preview-card.png">
    <meta name="twitter:description" content="Model Composition for Multimodal Large Language Models">
  </head>
  <body>
    <header class="page-header" role="banner">
      <font size=6><b>Model Composition for Multimodal Large Language Models</b></font>

      <!--<div style="margin-top: 1rem">

        <img src="/assets/static/logo.jpg" style="width:4em;vertical-align: middle" alt="Logo"/> <span style="vertical-align: middle"><font size=5>AgentForce Team</font></span><br>
      </div>-->

      <div style="margin-top: 2rem">
        <!-- <span class="author-block">Xiang Yue*<sup style="color:#6fbf73;">‚Ä†,1</sup>,</span> -->
        <span class="author-block">
          <font size=4><a href="https://carboncoo.github.io" style="text-decoration: none; color: inherit;">Chi Chen<sup>*,1</sup></a></font>,
        </span>
        <span class="author-block">
          <font size=4><a href="https://adu2021.github.io/" style="text-decoration: none; color: inherit;">Yiyang Du<sup>*,1</sup></a></font>,
        </span>
        <font size=4><span class="author-block">Zheng Fang<sup>1</sup>,</span></font>
        <font size=4><span class="author-block">Ziyue Wang<sup>1</sup>,</span></font>
        <font size=4><span class="author-block">Fuwen Luo<sup>1</sup>,</span></font>
        <span class="author-block">
          <font size=4><a href="https://www.lpeng.net/" style="text-decoration: none; color: inherit;">Peng Li<sup>2</sup>,</a></font>
        </span><br>
        <font size=4><span class="author-block">Ming Yan<sup>3</sup>,</span></font>
        <font size=4><span class="author-block">Ji Zhang<sup>3</sup>,</span></font>
        <font size=4><span class="author-block">Fei Huang<sup>3</sup>,</span></font>
        <font size=4><span class="author-block">Maosong Sun<sup>1</sup>,</span></font>
        <span class="author-block">
          <font size=4><a href="https://nlp.csai.tsinghua.edu.cn/~ly/" style="text-decoration: none; color: inherit;">Yang Liu<sup>1,2</sup></a></font>
        </span>
      
      </div>
      

      <div style="margin-top: 1rem">
        <font size=4><sup>*</sup> Equal Contribution </font><br>
        <font size=4><sup>1</sup> <a href="https://www.cs.tsinghua.edu.cn/csen/">Department of Computer Science and Technology, Tsinghua University</a></font><br>
        <font size=4><sup>2</sup> <a href="https://air.tsinghua.edu.cn/en/">Institute for AI Industry Research (AIR), Tsinghua University</a></font><br>
        <font size=4><sup>3</sup> Institute of Intelligent Computing, Alibaba Group</a></font><br>
      </div>
      
      <a href="https://arxiv.org/abs/2402.12750" class="btn"><b>üìù&emsp;Paper</b></a>
      
      <a href="" class="btn"><b>üí∞&emsp;Code for Model Composition</b></a>
      
      <a href="" class="btn"><b>üåê&emsp;Live Site</b></a>
    </header>

    <main id="content" class="main-content" role="main">
      <div style="text-align:center;margin-bottom: 2rem">
          <font size=4> Our proposed model composition method that creates a versatile model from existing MLLMs through a training-free and extensible process:</font>
          <br>
      </div>

      <div style="text-align:center;">
        <img src="/assets/static/fig-1.png" width="80%">
      </div>

      <!--<video controls width="100%" autoplay muted loop>
      <source src="assets/static/vid-1.mp4" type="video/mp4">
      </video>-->

      <h2 id="abstract">Abstract</h2>

<p>Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition
of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, <strong>NaiveMC</strong>, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce <strong>DAMC</strong> to address parameter interference and  mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose <strong>MCUB</strong>, a benchmark for assessing ability of MLLMs to understand inputs from diverse modalities. Experiments on this benchmark and four other multimodal understanding tasks show significant improvements over baselines, proving that model composition can create a versatile model capable of processing inputs from multiple modalities.</p>

<h2 id="principles-of-unified-alignment-for-agents">Principles of Unified Alignment for Agents</h2>

<div style="text-align:center;">
      <img src="assets/static/fig-2.png" width="95%" />
   </div>

<p>Agents, humans, and the environment are the three components that make up a working system of agents. To promote the orchestration of the three roles, the agents should work in the direction of eliminating the gap between agents and humans, agents and the environment, as well as adapting to the constraints imposed on the agents themselves. Based on this, we propose the principles of <strong>U</strong>nified <strong>A</strong>lignment for <strong>A</strong>gents (<strong>UA</strong>\(^2\)). To enumerate:</p>

<ul>
  <li>
    <p>The agents should align with <em>human intentions</em>. The agents need to correctly recognize the intentions of humans, including their user profiles, ambiguity in texts, as well as safety concerns. Aside from LLM alignment research which is formed of mostly a single prompt-response pair, specific challenges for agents lie in the nature of multi-turn interaction. Through multiple rounds of investigation, the agents should either elicit human preferences (<a href="https://arxiv.org/abs/2310.11589">Li et al., 2023</a>), or learning to self-correct from environmental feedback (<a href="https://arxiv.org/abs/2310.01798">Huang et al., 2023</a>), or both.</p>
  </li>
  <li>
    <p>The agents should align with <em>environmental dynamics</em>. To succeed in goal achievements, the agents should raise their awareness of the operation laws of the environment, the complexity of which emerges from the partial observability, time-variant property, stochasticity, etc. This is also advocated in (<a href="https://openreview.net/forum?id=BZ5a1r-kVsf">LeCun, 2022</a>) and (<a href="https://arxiv.org/abs/2312.05230">Hu &amp; Shu, 2023</a>) that propose to construct and incorporate a world model into an agent system.</p>
  </li>
  <li>
    <p>The agents should align with <em>self-constraints</em>. The underscored factor of current agent research and development is the constraints imposed on the agents themselves, including time and/or money budget limits. For foundation model agents, the underlying models (<em>e.g.</em>, proprietary LLMs/LMMs) are costly for inference, which hurdles the performance of agents in realistic scenarios.</p>
  </li>
</ul>

<h2 id="literature-review-from-the-lens-of-ua2">Literature Review from the Lens of UA\(^2\)</h2>

<p>According to the principles of <strong>UA</strong>\(^2\), we review the existing benchmarks and methods of agents and check if they suffice the development of agents in holistic real-world scenarios. Detailed analysis can be found in Section 3 of our <a href="https://arxiv.org/abs/2402.07744">paper</a>.</p>

<h3 id="benchmarks">Benchmarks</h3>

<p>For benchmarks, we review both the digital and the embodied ones, and consider the following three aspects:</p>

<ul>
  <li>
    <p><em>Human intentions</em>: Whether the authentic goals need to be inferred during task execution, or the intentions of humans are precisely conveyed in the descriptions.</p>
  </li>
  <li>
    <p><em>Environmental dynamics</em>: Whether the state transitions of the environment are intrinsically endowed with partial observability, temporality, or stochasticity.</p>
  </li>
  <li>
    <p><em>Self-constraints</em>: Whether the status of budgetary resources is reflected, including time consumption, the maximum number of actions or reasoning steps, etc.</p>
  </li>
</ul>

<p>The comparative review is summarized in the following table:</p>

<div style="text-align:center;">
      <img src="assets/static/table-1.png" width="95%" />
   </div>

<p>From the table, it is witnessed that existing benchmarks still cannot adequately cover all the three sources of alignments for agents. Specifically, in general, the development of digital benchmarks lags behind that of embodied benchmarks. This underscores the need for the construction of more comprehensive and realistic benchmarks, as well as fine-grained evaluation metrics that account for the principles of <strong>UA</strong>\(^2\).</p>

<h3 id="methods">Methods</h3>

<p>We continue to review representative agent methods from the perspective of <strong>UA</strong>\(^2\). For each method, we analyze whether it actively seeks alignment with <em>human intentions</em>, <em>environmental dynamics</em>, or <em>self-constraints</em>. The analysis is illustrated in the following figure:</p>

<div style="text-align:center;">
      <img src="assets/static/fig-3.png" width="95%" />
   </div>

<p>In our analysis, we categorize each methods according to the following criteria:</p>

<ul>
  <li>
    <p>By aligning with <em>human intentions</em>, the agents should coordinate with humans through reasoning or experience summarization. Candidate methods in this vein are HLA (<a href="https://arxiv.org/abs/2312.15224">Liu et al., 2023</a>), ExpeL (<a href="https://arxiv.org/abs/2308.10144">Zhao et al., 2023</a>), CAMEL (<a href="https://arxiv.org/abs/2303.17760">Li et al., 2023</a>), etc.</p>
  </li>
  <li>
    <p>By aligning with <em>environmental dynamics</em>, the agents ground themselves with external information from the environment. Example methods in this category are Reflexion (<a href="https://arxiv.org/abs/2303.11366">Shinn et al., 2023</a>), RAP (<a href="https://arxiv.org/abs/2305.14992">Hao et al., 2023</a>), CLIN (<a href="https://arxiv.org/abs/2310.10134">Majumder et al., 2023</a>), etc.</p>
  </li>
  <li>
    <p>By aligning with <em>self-constraints</em>, the agents should adopt an adaptive strategy in the process of task execution and/or group construction. Representative frameworks in this category are SwiftSage (<a href="https://arxiv.org/abs/2305.17390">Lin et al., 2023</a>), RetroFormer (<a href="https://arxiv.org/abs/2308.02151">Yao et al., 2023</a>), and DyLAN (<a href="https://arxiv.org/abs/2310.02170">Liu et al., 2023</a>). Finetuning a small-sized foundation model is also beneficial to the obedience of self-constraints, as some API calls of proprietary foundation models can be saved (examples: FireAct (<a href="https://arxiv.org/abs/2310.05915">Chen et al., 2023</a>) and AutoAct (<a href="https://arxiv.org/abs/2401.05268">Qiao et al., 2024</a>)).</p>
  </li>
</ul>

<p>Other basic techniques like ReAct (<a href="https://arxiv.org/abs/2210.03629">Yao et al., 2023</a>) are also the fundamental elements in most of advanced agent frameworks. Despite the emergence of diverse agent methodologies, plenty of room still exists for the unified alignment of agents with <em>human intentions</em>, <em>environmental dynamics</em>, and <em>self-constraints</em> simultaneously. A common case is that a method focuses too much on a single source of alignment, but meanwhile violating other sources severely (for example, when aligning with <em>environmental dynamics</em> by sampling a huge amount of trajectories, the agent aligns poorly with <em>self-constraints</em>). Therefore, elaborate agent framework design is required to strike a good balance of alignments with all the three roles. Detailed discussions can be found in our <a href="https://arxiv.org/abs/2402.07744">paper</a>.</p>

<h2 id="proof-of-concept-studies">Proof-of-Concept Studies</h2>

<p>To further validate the importance of <strong>UA</strong>\(^2\) in the design of both benchmarks and methods for agents, we upgrade WebShop with several realistic features according to the principles of <strong>UA</strong>\(^2\). We then initiate our agent design with <strong>UA</strong>\(^2\), and benchmark its performance as well as several candidate baselines on top of the retrofitted WebShop.</p>

<h3 id="environmental-construction">Environmental Construction</h3>

<p>We implement several realistic features on top of the <a href="https://webshop-pnlp.github.io/">WebShop</a> environment (<a href="https://arxiv.org/abs/2207.01206">Yao et al., 2022</a>) to reflect the three sources of alignment that agents should consider.</p>

<p>Try out the retrofitted WebShop at the live site <a href="http://49.232.144.86:5000">here</a>!</p>

<ul>
  <li>
    <p>The agent needs to analyze the user‚Äôs initial profile, tracking and inferring a series of shopping instructions. As user profiles and ambiguous intentions are integrated, this covers the alignment with <em>human intentions</em>:</p>

    <div style="text-align:center;">
    <img src="assets/static/human-intentions-r-webshop.gif" width="90%" />
 </div>
  </li>
  <li>
    <p>The search results in C-WebShop evolve with the historical click actions because of personalized reranking mechanisms. As the environment now becomes highly partially-observable, this covers the alignment with <em>environmental dynamics</em>:</p>

    <div style="text-align:center;">
    <img src="assets/static/environment-dynamics-r-webshop.gif" width="95%" />
 </div>
  </li>
  <li>
    <p>The agent needs to minimize its own monetary and time expenditures besides completing tasks. The implementation of runtime budgetary counter corresponds with the alignment with <em>self-constraints</em>:</p>

    <div style="text-align:center;">
    <img src="assets/static/self-constraints-r-webshop.gif" width="95%" />
 </div>
  </li>
</ul>

<p>Details can be found in Section 4.1 of our <a href="https://arxiv.org/abs/2402.07744"><strong>paper</strong></a>. We‚Äôve also open-sourced our <a href=""><strong>code</strong></a> for detailed setup if you want to deploy the environment locally.</p>

<h3 id="agent-design-and-experiments">Agent Design and Experiments</h3>

<p>Following the principles of <strong>UA</strong>\(^2\), we initiate our agent by introducing the structured memory module on top of ReAct. The introduced module is formed by two key components: <em>low-level</em> action insights and <em>high-level</em> intra-task experience. Basically, <em>low-level</em> insights are stored as the key actions extracted from the trajectories of successful runs; when new task arrives, the accumulated key action are to be retrieved as <em>high-level</em> experience to facilitate performance generalization.</p>

<p>The benchmarking results on the retrofitted WebShop are shown as follows:</p>

<div style="text-align:center;">
      <img src="assets/static/table-2-contents.png" width="95%" />
   </div>

<p>In the table, the averaged reward, success rate (SR) (%), the alignment gap (%) with human intentions (\(\mathbf{G}_\mathrm{HI}\)) and environment dynamics (\(\mathbf{G}_\mathrm{ED}\)), time (s) and money ($) cost of all methods are benchmarked in our retrofitted WebShop environment. The better performance under each metric is indicated by the darker <font color="green">green</font> shades. Still, check out our <a href="https://arxiv.org/abs/2402.07744"><strong>paper</strong></a> and <a href="https://github.com/AgentForceTeamOfficial/UA2-Agent"><strong>code</strong></a> for details =)</p>

<h2 id="actionable-insights">Actionable Insights</h2>

<p>Envisioning the future of autonomous agents powered by foundation models in real-world applications, in this section, we provide insights on the next steps of research from <strong>UA</strong>\(^{2}\):</p>

<ul>
  <li>
    <p><strong>Synergizing agents with alignment research.</strong> For example, one can leverage ideas like Constitutional AI (<a href="https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback">Bai et al., 2022</a>) to integrate the principles of unified alignment into the objectives of the agents.</p>
  </li>
  <li>
    <p><strong>Constructing realistic agent benchmarks.</strong> Taking <strong>UA</strong>\(^{2}\) into account, we advocate for more realistic benchmark design that reflects the intricate real-world scenarios.</p>
  </li>
  <li>
    <p><strong>Developing holistic evaluations for agents.</strong> The dissection of the agent performance is necessary to assess the obedience of each principle of <strong>UA</strong>\(^{2}\).</p>
  </li>
  <li>
    <p><strong>Toward self-evolving agents through continual alignment.</strong> In the spirit of <strong>UA</strong>\(^{2}\), we envision the future where an agent can improve upon itself through continual interaction with both human users and environment, leading to better use and efficiency.</p>
  </li>
</ul>

<h2 id="contact">Contact</h2>

<p>This project is co-led by <a href="https://minicheshire.github.io">Zonghan Yang</a> (yangzh20@mails.tsinghua.edu.cn), <a href="https://github.com/xxmlala">An Liu</a> (la22@mails.tsinghua.edu.cn), and <a href="https://github.com/BBQGOD">Zijun Liu</a> (liuzijun20@mails.tsinghua.edu.cn), and is advised by <a href="https://www.lpeng.net/">Peng Li</a> (lipeng@air.tsinghua.edu.cn) and <a href="https://nlp.csai.tsinghua.edu.cn/~ly">Yang Liu</a> (liuyang2011@tsinghua.edu.cn).</p>

<h2 id="citation">Citation</h2>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{yang2024unified,
  author        = {Yang, Zonghan and Liu, An and Liu, Zijun and Liu, Kaiming and Xiong, Fangzhou and Wang, Yile and Yang, Zeyuan and Hu, Qingyuan and Chen, Xinrui and Zhang, Zhenhe and Luo, Fuwen and Guo, Zhicheng and Li, Peng and Liu, Yang},
  title         = {Towards Unified Alignment Between Agents, Humans, and Environment},
  year          = {2024},
  eprint        = {2402.07744},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI}
}
</code></pre></div></div>


      <!-- <footer class="site-footer">
        <span style="vertical-align: middle"><b>This project is powered by</b></span> <img src="/assets/static/logo.jpg" style="width:3em;vertical-align: middle" alt="Logo"/> <span style="vertical-align: middle"><b>AgentForce Team, affiliated with <a href="https://www.cs.tsinghua.edu.cn/csen/">DCST</a> and <a href="https://air.tsinghua.edu.cn/en/">AIR</a> in Tsinghua University. The environment construction in the proof-of-concept studies is heavily inspired by the original <a href="https://webshop-pnlp.github.io/">WebShop</a> environment.</b></span>
        </span>
      </footer> -->
    </main>
  </body>
</html>